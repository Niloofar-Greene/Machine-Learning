summary(data_credits)
class(data_credits)
####### ############################ Bagging ################################
#help(bagging)
# Y = class, . = toutes autres variables. Donc, ca veut dire, class en fonction de tous.
# df= data_credit, nombre d'arbres = mfinal = 100
# pour le control : maxdepth = max de profondeur et maxsplit = split des variables X quand elles sont quantitatives
bagging_credit=bagging(class~.,data_credits,mfinal=10)
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
#data_credits=read.csv("C:\\Users\\hmensah\\Documents\\Documents\\Cours_IED\\data_credit.csv",sep=';',colClasses ='factor')
data_credits=data_credit <- read_delim("data_credit.csv",";", escape_double = FALSE, trim_ws = TRUE)
head(data_credits)
head(data_credits$class)
data_credits=data_credit <- read_delim("data_credit.csv",";", escape_double = FALSE, trim_ws = TRUE)
data_credits= read_delim("data_credit.csv",";", escape_double = FALSE, trim_ws = TRUE)
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
View(data_credits)
data_credit <- read_csv("data_credit.csv")
data_credits <- read_csv("data_credit.csv")
View(data_credits)
data_credits[1]
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_credits[1]=factor(data_credits[1])
summary(data_credits)
factor(data_credits[2])
data_credits$job=factor(data_credits[2])
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
summary(bagging_credit)
summary(data_credits)
setwd("~/GitHub/Machine-Learning/Devoir final")
############################################### Importer le fichier de data ############################################
library(readr)
df <- read_delim(" data_examen_final.csv", ";", escape_double = FALSE, trim_ws = TRUE)
############################################### Nettoyage de donnees ############################################
#remove the extra characters in column name by changing the column names
colnames(df)= gsub('"','',colnames(df))
#remove extra characters in age column
cols <- c("age", "job", "marital","default","education","housing","loan","contact","month","poutcome","y")
df[cols] <- lapply(df[cols], function(x) gsub('"','', x))
# est-ce que chaque client est repete une seule fois ou plus ?
A=df[(duplicated(df)) , ]
# la reponse est non
rm(A)
############################################### Repondre aux questions ############################################
# 1)	Pouvez-vous determiner l'age moyen ainsi que la mediane concernant la balance du compte de notre clientele ?
summary(df)
# on se rend compte que la variable age est character donc on doit le convertir en numerique
df$age=as.numeric(df$age)
# calcul de moyenne et median de l'age
age_mean= mean(df$age)
age_madian=median(df$age)
# 2)	Dans la base de donnÃ©es se trouve la variable y reprÃ©sentant si la personne aÌ souscrit au dÃ©pÃ´t direct.
# Pouvez-vous changer le nom de cette variable en âdepositâ ?
a=grep("^y", colnames(df))
names(df)[a]="deposit"
# 3) Veuillez creer la table de donnÃ©es data_bank_deposit qui ne possÃ¨dera que les clients qui ont une balance strictement supÃ©rieure aÌ 0.
data_bank_deposit=subset(df, balance > 0)
# 4) un arbre de classification sur la table de donnÃ©es data_bank_deposit en prenant toutes les variables explicatives.
# Cet arbre de classification sera un arbre aÌ base de la mesure de Gini, possÃ©dant une profondeur maximale de 5 ainsi quâun
# nombre minimal de 50 observations dans les feuilles terminales.
# Veuillez effecteur un tel arbre de classification et afficher le graphique de l'arbre en question.
library(rpart)
rpart_bank_deposit=rpart(deposit~., data_bank_deposit, parms = list(split='gini'), control=list(maxdepth=5, minsplit=50) , method ='class')
# affichier en graphique la presentation de modele
par(mar=c(0,2,2,0))
plot(rpart_bank_deposit)
text(rpart_bank_deposit)
library(rattle)
fancyRpartPlot(rpart_bank_deposit, caption = NULL)
# 5)	Pouvez interprÃ©ter la regle menant a la premiere feuille terminale
rpart_bank_deposit
# 6)	citer les 3 variables explicatives ayant la plus grande importance dans la construction de l'arbre.
rpart_bank_deposit$variable.importance[order(rpart_bank_deposit$variable.importance, decreasing = TRUE)]
# 7) Veuillez dÃ©terminer le taux naturel d'erreur que nous possedons dans notre table de donnÃ©es data_bank_deposit.
#Prediction sur toute notre table de donnees
# yes % = TNE (taux naturel erreur)
table(data_bank_deposit$deposit)/nrow(data_bank_deposit)
# 8)	Veuillez construire trois modÃ¨les, boosting, bagging, et foret alÃeatoire tous de 50 arbres et ayant une
# profondeur maximale de 4.
# Pour ce qui est de la forÃªt alÃ©atoire, veuillez choisir un tirage alÃ©atoire de variables de 5.
library(adabag)
############################################### Importer le fichier de data ############################################
library(readr)
df <- read_delim(" data_examen_final.csv", ";", escape_double = FALSE, trim_ws = TRUE)
df <- read_delim(" data_examen_final.csv", ";", escape_double = FALSE, trim_ws = TRUE)
df <- read_delim("data_examen_final.csv", ";", escape_double = FALSE, trim_ws = TRUE)
############################################### Nettoyage de donnees ############################################
#remove the extra characters in column name by changing the column names
colnames(df)= gsub('"','',colnames(df))
#remove extra characters in age column
cols <- c("age", "job", "marital","default","education","housing","loan","contact","month","poutcome","y")
df[cols] <- lapply(df[cols], function(x) gsub('"','', x))
# est-ce que chaque client est repete une seule fois ou plus ?
A=df[(duplicated(df)) , ]
# la reponse est non
rm(A)
############################################### Repondre aux questions ############################################
# 1)	Pouvez-vous determiner l'age moyen ainsi que la mediane concernant la balance du compte de notre clientele ?
summary(df)
# on se rend compte que la variable age est character donc on doit le convertir en numerique
df$age=as.numeric(df$age)
# calcul de moyenne et median de l'age
age_mean= mean(df$age)
age_madian=median(df$age)
# 2)	Dans la base de donnÃ©es se trouve la variable y reprÃ©sentant si la personne aÌ souscrit au dÃ©pÃ´t direct.
# Pouvez-vous changer le nom de cette variable en âdepositâ ?
a=grep("^y", colnames(df))
names(df)[a]="deposit"
# 3) Veuillez creer la table de donnÃ©es data_bank_deposit qui ne possÃ¨dera que les clients qui ont une balance strictement supÃ©rieure aÌ 0.
data_bank_deposit=subset(df, balance > 0)
# 4) un arbre de classification sur la table de donnÃ©es data_bank_deposit en prenant toutes les variables explicatives.
# Cet arbre de classification sera un arbre aÌ base de la mesure de Gini, possÃ©dant une profondeur maximale de 5 ainsi quâun
# nombre minimal de 50 observations dans les feuilles terminales.
# Veuillez effecteur un tel arbre de classification et afficher le graphique de l'arbre en question.
library(rpart)
rpart_bank_deposit=rpart(deposit~., data_bank_deposit, parms = list(split='gini'), control=list(maxdepth=5, minsplit=50) , method ='class')
# affichier en graphique la presentation de modele
par(mar=c(0,2,2,0))
plot(rpart_bank_deposit)
text(rpart_bank_deposit)
library(rattle)
fancyRpartPlot(rpart_bank_deposit, caption = NULL)
# 5)	Pouvez interprÃ©ter la regle menant a la premiere feuille terminale
rpart_bank_deposit
# 6)	citer les 3 variables explicatives ayant la plus grande importance dans la construction de l'arbre.
rpart_bank_deposit$variable.importance[order(rpart_bank_deposit$variable.importance, decreasing = TRUE)]
# 7) Veuillez dÃ©terminer le taux naturel d'erreur que nous possedons dans notre table de donnÃ©es data_bank_deposit.
#Prediction sur toute notre table de donnees
# yes % = TNE (taux naturel erreur)
table(data_bank_deposit$deposit)/nrow(data_bank_deposit)
library(adabag)
summary(data_bank_deposit)
class(data_bank_deposit)
# et type de table doit etre data.frame
data_bank_deposit=data.frame(data_bank_deposit)
class(data_bank_deposit)
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_bank_deposit$deposit=factor(data_bank_deposit$deposit)
data_bank_deposit$job=factor(data_bank_deposit$job)
data_bank_deposit$marital=factor(data_bank_deposit$marital)
data_bank_deposit$education=factor(data_bank_deposit$education)
data_bank_deposit$housing=factor(data_bank_deposit$housing)
data_bank_deposit$loan=factor(data_bank_deposit$loan)
data_bank_deposit$contact=factor(data_bank_deposit$contact)
data_bank_deposit$month=factor(data_bank_deposit$month)
data_bank_deposit$poutcome=factor(data_bank_deposit$poutcome)
data_bank_deposit$default=factor(data_bank_deposit$default)
summary(data_bank_deposit)
View(rpart_bank_deposit)
View(data_bank_deposit)
summary(data_bank_deposit)
setwd("~/GitHub/Machine-Learning/Seance 9")
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
factor(data_credits$checking_status)
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_credits$checking_status=factor(data_credits$checking_status)
summary(data_credits)
factor_function <- function(a) {
for(i in 1:a) {
data_credits[,i] <- factor(data_credits[,i])
}
}
ncol(data_credits)
factor_function(ncol(data_credits))
factor(data_credits[,2])
data_credits[,2]=factor(data_credits[,2])
summary(data_credits)
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
####### ############################ Bagging ################################
#help(bagging)
# Y = class, . = toutes autres variables. Donc, ca veut dire, class en fonction de tous.
# df= data_credit, nombre d'arbres = mfinal = 100
# pour le control : maxdepth = max de profondeur et maxsplit = split des variables X quand elles sont quantitatives
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_credits$checking_status=factor(data_credits$checking_status)
data_credits$duration=factor(data_credits$duration)
summary(data_credits)
library(dplyr)
iris_char <- iris %>%
mutate(Species=as.character(Species),
char_column=sample(letters[1:5], nrow(iris), replace=TRUE))
View(iris_char)
iris
summary(iris)
summary(iris_char)
letters[1:5]
View(iris_char)
data_credits=as.factor(data_credits)
summary(data_credits)
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
iris_factor <- iris_char %>%
mutate_if(sapply(iris_char, is.character), as.factor)
View(iris_factor)
summary(iris_factor)
iris_factor <- data_credits %>%
mutate_if(sapply(data_credits, is.character), as.factor)
summary(iris_factor)
# et type de table doit etre data.frame
data_credits=data.frame(data_credits)
class(data_credits)
bagging_credit=bagging(class~.,data_credits,mfinal=10)
library(dplyr)
iris_char <- iris %>%
mutate(Species=as.character(Species),
char_column=sample(letters[1:5], nrow(iris), replace=TRUE))
iris_factor <- iris_char %>%
mutate_if(sapply(iris_char, is.character), as.factor)
sapply(iris_factor, class)
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
################################### Bagging ################################
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_credits <- data_credits %>% mutate_if(sapply(data_credits, is.character), as.factor)
summary(data_credits)
# et type de table doit etre data.frame
data_credits=data.frame(data_credits)
class(data_credits)
bagging_credit=bagging(class~.,data_credits,mfinal=10)
summary(bagging_credit)
# pour voir la classe predites
bagging_credit$class
# pour TMC - on fait table croise avec table predit et table originale (obs de class)
# pour 33 cas, modele a predit "bad" alors que en realite etait "good" - 667 sont predit comme good et sont good en realite
table(bagging_credit$class, data_credit$class)
# pour TMC - on fait table croise avec table predit et table originale (obs de class)
# pour 33 cas, modele a predit "bad" alors que en realite etait "good" - 667 sont predit comme good et sont good en realite
table(bagging_credits$class, data_credit$class)
# pour TMC - on fait table croise avec table predit et table originale (obs de class)
# pour 33 cas, modele a predit "bad" alors que en realite etait "good" - 667 sont predit comme good et sont good en realite
table(bagging_credit$class, data_credits$class)
# les obs bien predites par le modele
diag(table(bagging_credit$class, data_credit$class))
# les obs bien predites par le modele
diag(table(bagging_credits$class, data_credit$class))
# les obs bien predites par le modele
diag(table(bagging_credit$class, data_credits$class))
# le nombre total ou le modele a bien classifie
SUMM=sum(diag(table(bagging_credit$class, data_credit$class)))
# le nombre total ou le modele a bien classifie
SUMM=sum(diag(table(bagging_credit$class, data_credits$class)))
# taux de bonne classification - 83.6%
TBC=SUMM/nrow(data_credits)
# TMC = 16.4%
TMC= 1-TBC
# Importance des variables Bagging
bagging_credit$importance
#IMPORTANCE DES VARIABLES (poids) - checking status a le poids le plus eleve de 31,89
bagging_credit$importance
library(readr)
library(rpart)
library(adabag) # Pour le Bagging et le Boosting
library(randomForest) # Pour les forets aleatoires
data_credits <- read_csv("data_credit.csv")
head(data_credits)
head(data_credits$class)
########################################
summary(data_credits)
class(data_credits)
# Calculer notre d'erreur naturel# est ce que la personne est un mauvais paiyeur ou non. mauvais payeur = 1, sinon = 0
# bad % = TNE (taux naturel erreur)
table(data_credits$class)/nrow(data_credits)
#appliquerle modele de classification - gini
rpart_credit_gini=rpart(class~.,data_credits,parms = list(split='gini'),control = list(maxdepth=10))
#appliquerle modele de classification - entropie
rpart_credit_entropy=rpart(class~.,data_credits,parms = list(split='information'),control = list(maxdepth=3))
#sommaire du modele
summary(rpart_credit_gini)
#affiche en graphique la representation du modele
plot(rpart_credit_gini)
text(rpart_credit_gini)
#afficher les regle
rpart_credit_gini
#Prediction sur toute notre table de donnees
y_pred=predict(rpart_credit_gini,data_credits)
y_pred #probabilite
#Si la probabilite pour good est superieur a 50%, dans ce cas cela sera la valeur good et sinon ca sera la valeur bad
y_class=ifelse(data.frame(y_pred)$good>0.5,'good','bad')
#Tableau croise pour en deduire le taux de mauvaise classification
table(y_class,data_credits$class)/nrow(data_credits)
############################################ Methode d'ensemble ############################################
# Calculer notre d'erreur naturel
table(data_credits$class)/nrow(data_credits)
TNE=table(data_credits$class)/nrow(data_credits)
################################### Bagging ################################
# pour bagging, boosting et foret aleatoire les colonnes de character doivent etre des facteurs
data_credits <- data_credits %>% mutate_if(sapply(data_credits, is.character), as.factor)
summary(data_credits)
# et type de table doit etre data.frame
data_credits=data.frame(data_credits)
class(data_credits)
bagging_credit=bagging(class~.,data_credits,mfinal=100)
summary(bagging_credit)
# pour voir la classe predites
bagging_credit$class
# pour TMC - on fait table croise avec table predit et table originale (obs de class)
# pour 33 cas, modele a predit "bad" alors que en realite etait "good" - 667 sont predit comme good et sont good en realite
table(bagging_credit$class, data_credits$class)
# les obs bien predites par le modele
diag(table(bagging_credit$class, data_credits$class))
# le nombre total ou le modele a bien classifie
SUMM=sum(diag(table(bagging_credit$class, data_credits$class)))
# taux de bonne classification - 83.6%
TBC=SUMM/nrow(data_credits)
# TMC = 16.4%
TMC= 1-TBC
# TMC = 16.4% vs. TNE = 30%
#IMPORTANCE DES VARIABLES (poids) - checking status a le poids le plus eleve de 31,89
bagging_credit$importance
####### ############################ Boosting ################################
boosting_credit=boosting(class~.,data_credits,mfinal=101,control=rpart.control(maxdepth=5))
summary(boosting_credit)
# Importance des variables Boosting
# poids des arbres. 0.55 est le meilleurs
boosting_credit$weights
# les obs bien predites par le modele
diag(table(boosting_credit$class, data_credit$class))
# le nombre total ou le modele a bien classifie
SUMM=sum(diag(table(boosting_credit$class, data_credit$class)))
# taux de bonne classification
TBC=SUMM/nrow(data_credits)
# TMC Boosting
# TMC = 4.3% vs TNE = 30%
TMC= 1-TBC
# variables important --> purpose
boosting_credit$importance
rf_credit=randomForest(class~.,data_credits,ntree=101,mtry=5)
# predicted (au lieu de class), err_rate, obb_times : # de temps que la personne qui etait pas tirees dans le OBB,
# importance, y = var cible
summary(rf_credit)
1-sum(diag(table(rf_credit$predicted, data_credits$class)))/nrow(data_credits)
# importance - checking_status
rf_credit$importance
#### TMC des 3 methodes - bosting est best
1-sum(diag(table(bagging_credit$class, data_credits$class)))/nrow(data_credits)
1-sum(diag(table(boosting_credit$class, data_credits$class)))/nrow(data_credits)
1-sum(diag(table(rf_credit$predicted, data_credits$class)))/nrow(data_credits)
boosting_credit=boosting(class~.,data_credits,mfinal=101,control=rpart.control(maxdepth=3))
